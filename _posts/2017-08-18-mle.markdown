---
layout: post
title:  "Maximum Likelihood Estimation of Gaussian Parameters"
date:   2017-08-18
categories: mle
comments: True
---


## Maximum Likelihood Estimation of Gaussian Parameters $$[\mu,\Sigma]$$


### The Big Picture
  
Practically speaking, we have data with labels, and we want to take some new data, and classify it with the labels from the old data. For example, in the below images, we see (LEFT) data with two labels, and (RIGHT) unlabeled data. We want to be able to categorize each point from the new data as belonging to either the `blue` group or the `yellow` group.

<img src="/misc/figs/mle_1.png" align="left" style="width: 350px;"/>
<img src="/misc/figs/mle_2.png" align="right" style="width: 350px;"/>


Labeled training data (\textsc{left}) and unlabeled new data (\textsc{right})


Labeling dots as either `blue` or `yellow` sounds pretty boring, but the same idea applies to labeling emails as `spam` or `regular` or classifying audio clips as the vowel `[a]` or the vowel `[o]`.

To accomplish that task, we build a statistical model, learning its shape from the old labeled data. For example, for the above data we could build two models (eg. GMMs), a `blue` model and a `yellow` model, and then see which model is more similar to a new data point. Another approach would be to build a single model (eg. a neural net) that distinguishes `blue` points from `yellow` points, and then see how it categorizes a new point. Here, we're working here with the former approach (build two models).


<img src="/misc/figs/mle_model_2.png" align="right" style="width: 350px;"/>
<img src="/misc/figs/mle_model_1.png" align="left" style="width: 350px;"/>


Yellow Model (\textsc{left}) Blue Model (\textsc{right})

We assume the data was in a sense _generated_ by some process, and we're try to model what that process was. This is called the generative approach. The model we're trying to learn is an approximation of the underlying process that created the data in the first place. So our data is just a sample from a process, and we want to learn the process.

At the end of the day, once we have our two models, we will use them to find which model was more likely to have _generated_ a new data point. To take the leap from data $$\rightarrow$$ model, we need to be able to not only estimate the parameters of the model (eg. for Gaussians we need $$[\mu, \Sigma]$$), but we want to estimate them to get the BEST model possible for our data. That's were MLE comes into play.


### MLE

MLE is one flavor of parameter estimation in machine learning, and in to perform parameter estimation, we need:

1. some data $$\mathbf{X}$$
2. some hypothesized generating function of the data $$f(\mathbf{X},\theta)$$
3. a set of parameters from that function $$\theta$$
4. some evaluation of the goodness of our parameters (an objective function)

In MLE, the objective function (evaluation) we chose is the _likelihood_ of the data given our model. This intuitively makes sense if you keep in mind that we don't get to change our data, and we have to make some assumption about the form of our model, but we CAN adjust the parameterization of our model. So, we are limited to adjusting $$\theta$$, and we might as well choose the best $$\theta$$ for our data. To find the best $$\theta$$ then, find the $$\theta$$ with maximizes our evaluation function (the likelihood). Therefore, in its general form the MLE is:


$$\theta_{MLE} = \underset{\theta}{\operatorname{argmax}} p(\mathbf{X}|\theta)$$



### MLE for a Gaussian

We assume the data we're working with was generated by an underlying Gaussian process in the real world. As such, the likelihood function is the Gaussian itself.


$$\begin{align}
p(\mathbf{X}|\theta) &= \mathcal{N}(\mathbf{X}|\theta)\\
              &= \mathcal{N}(\mathbf{X}|\mu, \Sigma)\\
\end{align}$$


Therefore, for MLE of a Gaussian model, we will need to find good estimates of both parameters: $$\mu$$ and $$\Sigma$$:


$$\begin{align}
  \mu_{MLE} = \underset{\mu}{\operatorname{argmax}} \mathcal{N}(\mathbf{X}|\mu, \Sigma)\\
  \Sigma_{MLE} = \underset{\Sigma}{\operatorname{argmax}} \mathcal{N}(\mathbf{X}|\mu, \Sigma)
\end{align}$$


Solving these two above equations to find the best $$\mu$$ and $$\Sigma$$ is a job for our good old friends from calculus... partial derivatives! 

Before we can get to the point where we can find our best $$\mu$$ and $$\Sigma$$, we need to do some algebra, and to make that algebra easier, instead of just using the likelihood function as our evaluation function, we're going to use the log likelihood. This makes the math easier and it doesn't run any risks of giving us worse results. That's because the $$\log()$$ function is monotonically increasing, and therefore


$$\begin{align}
  \underset{\theta}{\operatorname{argmax}} \log(f(\theta)) == \underset{\theta}{\operatorname{argmax}} f(\theta)
\end{align}$$


So now, we know that we want to get the best parameters $$\theta = [\mu, \Sigma]$$ for a dataset $$\mathbf{X}$$ evaluating on a normal, Gaussian distribution. 


$$\begin{align}
  \theta_{MLE} &= \underset{\theta}{\operatorname{argmax}} \log(\mathcal{N}(\mathbf{X}|\theta)) \\
\end{align}$$


Since in reality our dataset $$\mathbf{X}$$ is a set of labeled data $$\mathbf{X} = [\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3} \ldots \mathbf{x}_{n}]$$, to evaluate our parameters on the entire dataset, we need to sum up the log likelihood for each data point.


$$\begin{align}
  \log(\mathcal{N}(\mathbf{X}|\theta)) &= \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\theta)) \\
\end{align}$$


Remember how that $$\theta$$ is a general catch-all for any set of parameters? Let's be more explicit with our Gaussian parameters $$[\mu, \Sigma]$$:


$$\begin{align}
  \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\theta)) &= \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\mu, \Sigma))\\
\end{align}$$




Here we're going to make a big simplfying assumption (and in reality a pretty common one). We're going to assume that our Gaussians have diagonal covariance matrices. So the full covariance matrix $$\mathbf{\Sigma}$$ gets replaced by a diagonal variance vector $$\sigma^{2}$$:


$$\begin{align}
  \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\mu, \Sigma)) &= \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\mu, \sigma^{2}))
\end{align}$$


Now, with this simplification, we can take a look at now our fully specified log likelihood function that we'll be working with from here on out. 


$$\begin{align}
  \sum\limits_{n=1}^{N} \log(\mathcal{N}(\mathbf{x}_{n}|\mu, \sigma^{2})) &= \sum\limits_{n=1}^{N} \log \Big( \frac{1}{\sqrt{2\pi\sigma^{2}}} \cdot \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \\
\end{align}$$


Now we have the likelihood as we want it (Gaussian, logged, diagonal covariance matrix). Let's not forget what our main goal is! We want to find the best parameters for our model given our data, so we're going to find the $$\underset{\mu}{\operatorname{argmax}}$$ and $$\underset{\sigma^{2}}{\operatorname{argmax}}$$. Before we can get to that point, we need to do some simplifications to the log likelihood to make it easier to work with (that is, since we will soon be doing some partial derivatives, the log likelihood in its current form it will lead to some messy math). In the following, $$\mathcal{LL}$$ means _log likelihood_.

The next first steps take advantage of our choice to use the log likelihood instead of the plain likelihood. Our first step will be to use the log product rule:

$$\begin{align}
  \mathcal{LL} &= \sum\limits_{n=1}^{N} \log \Big( \frac{1}{\sqrt{2\pi\sigma^{2}}} \cdot \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \\
  &= \sum\limits_{n=1}^{N} \Big( \log \Big( \frac{1}{\sqrt{2\pi\sigma^{2}}} \Big) + \log \Big( \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \Big) \\
\end{align}$$


Now we will use the log quotient rule:

$$\begin{align}
  \mathcal{LL} &= \sum\limits_{n=1}^{N} \Big( \log \Big( \frac{1}{\sqrt{2\pi\sigma^{2}}} \Big) + \log \Big( \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \Big) \\
               &= \sum\limits_{n=1}^{N} \Big( \log(1) - \log \big( \sqrt{2\pi\sigma^{2}} \big) + \log \Big( \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \Big)\\
\end{align}$$

Now, we'll use the log power rule:

$$\begin{align}
  \mathcal{LL} &=  \sum\limits_{n=1}^{N} \Big( \log(1) - \log \big( \sqrt{2\pi\sigma^{2}} \big) + \log \Big( \exp^{-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}}\big)}  \Big) \Big)\\
               &= \sum\limits_{n=1}^{N} \Big( \log(1) - \log \big( \sqrt{2\pi\sigma^{2}} \big) +
                  \Big(-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \big) \cdot \log(e) \Big) \Big)\\
\end{align}$$

We're now going to be explicit that the $$\log()$$ function we used was base $$e$$. This allows use to simplify $$\log_{e}(e) = 1$$ as well as $$\log(1) = 0$$ (regardless of base).

$$\begin{align}
  \mathcal{LL} &= \sum\limits_{n=1}^{N} \Big( \log(1) - \log \big( \sqrt{2\pi\sigma^{2}} \big) +
  \Big(-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \big) \cdot \log(e) \Big) \Big) \\
  &= \sum\limits_{n=1}^{N} \Big( - \log \big( \sqrt{2\pi\sigma^{2}} \big) +
  \Big(-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \big) \Big) \Big)\\
\end{align}$$

We can apply the power rule one more time (remember that $$\sqrt{x} = x^{1/2}$$).

$$\begin{align}
  \mathcal{LL} &= \sum\limits_{n=1}^{N} \Big( - \log \big( \sqrt{2\pi\sigma^{2}} \big) +
  \Big(-\frac{1}{2} \big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \big) \Big) \Big)\\
  &= \sum\limits_{n=1}^{N} \Big(- \frac{1}{2} \cdot \log ( 2\pi\sigma^{2} )  -\frac{1}{2} \Big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \Big) \Big) \\
\end{align}$$

Now for some basic algebra simplification:

$$\begin{align}
  \mathcal{LL} &= \sum\limits_{n=1}^{N}  \Big( - \frac{1}{2} \log ( 2\pi\sigma^{2} )  -\frac{1}{2} \Big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \Big) \Big)\\
  &= - \frac{N}{2} \log ( 2\pi\sigma^{2} )  + \sum\limits_{n=1}^{N} -\frac{1}{2} \Big(\frac{(x_{n} - \mu)^{2}}{\sigma^{2}} \Big) \\
  &= - \frac{N}{2} \log ( 2\pi\sigma^{2} )  -\frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \\
\end{align}$$

Now we have our log likelihood function ($$\mathcal{LL}$$) in a nice, easy to work with form. So far all we've done is simplify the original equation, we haven't even started trying to find our best parameters. Remember how we started with the goal that we want to find the best parameters via $$\operatorname{argmax}()$$? Well, now were going to do that.

To put it in another way, we're doing MLE: Maximum Likelihood Estimation. All we've done so far is get the _likelihood_ in a nice form, now we need to _estimate_ it's _maximum_ for our parameters. We've got the $$\mathcal{LL}$$ ready for $$\underset{\theta}{\operatorname{argmax}}(\mathcal{LL})$$, now we need to do the $$\underset{\theta}{\operatorname{argmax}}$$ part. This is where we get a little help from our friends, partial derivatives. We need _partial_ derivatives because our $$\theta$$ is really two variables $$[\mu, \sigma^{2}]$$, and we need the best value for each.

So, now we're going to solve the problem for each variable one-by-one:

$$\begin{align}
  \underset{\mu}{\operatorname{argmax}} \mathcal{LL}(X|\mu, \sigma^{2})\\
  \underset{\sigma^{2}}{\operatorname{argmax}} \mathcal{LL}(X|\mu, \sigma^{2})
\end{align}$$

To get the $${\operatorname{argmax}}$$ for each parameter we have to do two things. First, we must (1) derive the partial derivative of the function with respect to that parameter, and then (2) set that partial derivative to zero, and solve for our parameter.

By doing step (1), we get an equation for the change of the function, and we know that if the function isn't changing at a certain point, that point is a maximum or a minimum. Since we're working with a Gaussian function, we already know ahead of time that there is one and only one maximum, and there is no minimum! Easy as that!

### MLE of $$\mu$$

First we'll work to solve for the mean of our Gaussian, $$\mu$$. Remember we've got our likelihood function in a simple form:

$$\begin{align}
  \mathcal{LL} = - \frac{N}{2} \log ( 2\pi\sigma^{2} )  - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \\
\end{align}$$

and now we want to get the best $$\mu$$ for that function:

$$\begin{align}
  \underset{\mu}{\operatorname{argmax}} \mathcal{LL}(X|\mu, \sigma^{2}) := \frac{\partial \mathcal{LL}}{\partial \mu} = 0
\end{align}$$

So, to get to the point where we can set the partial derivative to zero and solve, we need to first find the partial derivative with respect to $$\mu$$:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} = \frac{\partial}{\partial \mu} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} )  - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Now let's start simplifying! First we can right off the bat get rid of the first term since it doesn't contain $$\mu$$, and therefore is practically speaking a constant:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &= \frac{\partial}{\partial \mu} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} )  - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= \frac{\partial}{\partial \mu} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} ) \Big)  + \frac{\partial}{\partial \mu} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= 0 + \frac{\partial}{\partial \mu} \Big(- \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= \frac{\partial}{\partial \mu} \Big(- \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Next, remember that the summation expression is just a convenient way to write a longer expression:


$$\begin{align}
\sum\limits_{n=1}^{N} f(x_{n}) &= f(x_{1}) + f(x_{2}) + \ldots + f(x_{N})
\end{align}$$

Also, We know from the summation rule that :

$$\begin{align}
\frac{\partial}{\partial x}  \big( f(x) + g(x) \big) &= \frac{\partial}{\partial x}f(x) + \frac{\partial}{\partial x}g(x)
\end{align}$$

Therefore, when we take the derivative of a sum, we can reformulate it as a sum of derivatives:

$$\begin{align}
\frac{\partial}{\partial x} \sum\limits_{n=1}^{N} f(x_{n}) &= \sum\limits_{n=1}^{N} \frac{\partial}{\partial x} f(x_{n}) 
\end{align}$$

Now, getting back to the problem at hand, we can move the derivative operator inside the summation term:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &= \frac{\partial}{\partial \mu} \Big(- \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= \frac{\partial}{\partial \mu} \Big( \sum\limits_{n=1}^{N} - \frac{1}{2\sigma^{2}} (x_{n} - \mu)^{2} \Big)\\
  &= \sum\limits_{n=1}^{N} \frac{\partial}{\partial \mu} \Big(- \frac{1}{2\sigma^{2}} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Now we can use the product rule:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &= \sum\limits_{n=1}^{N} \frac{\partial}{\partial \mu} \Big(- \frac{1}{2\sigma^{2}} \cdot (x_{n} - \mu)^{2} \Big)\\
  &= \sum\limits_{n=1}^{N}\Big( \frac{\partial}{\partial \mu} \big(- \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} + \big(- \frac{1}{2\sigma^{2}} \big) \cdot \frac{\partial}{\partial \mu} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Now some terms will nicely drop out:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &= \sum\limits_{n=1}^{N}\Big( \frac{\partial}{\partial \mu} \big(- \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} + \big(- \frac{1}{2\sigma^{2}} \big) \cdot \frac{\partial}{\partial \mu} (x_{n} - \mu)^{2} \Big)\\
  &= \sum\limits_{n=1}^{N}\Big( 0 + \big(- \frac{1}{2\sigma^{2}} \big) \cdot \frac{\partial}{\partial \mu} (x_{n} - \mu)^{2} \Big)\\
  &= - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} \frac{\partial}{\partial \mu} (x_{n} - \mu)^{2}\\
\end{align}$$

At this point we can use the chain rule, $$\frac{\partial}{\partial x}\big(f(g(x))\big) = \frac{\partial}{\partial x}f(g(x)) \cdot \frac{\partial}{\partial x}g(x)$$, with $$g(x) = (x - \mu)$$ and $$f(x) = x^{2}$$.

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &= - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} \frac{\partial}{\partial \mu} (x_{n} - \mu)^{2}\\
  &= - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} 2 (x_{n} - \mu) \cdot -1 \\
  &=  \frac{1}{\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu) \\
\end{align}$$

Yay! We've done as much simplifying as we can at this point, and gotten rid of all of our $$\frac{\partial \mathcal{LL}}{\partial \mu}$$ terms!

Now what we have is the simplest form of the partial derivative of our likelihood function with respect to $$\mu$$. Now we want to use this equation to find the best $$\mu$$, so we set it equal to zero, and solve for $$\mu$$.

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \mu} &=  \frac{1}{\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu) \\
  0 &=  \frac{1}{\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu) \\
  0 &=  \sum\limits_{n=1}^{N} (x_{n} - \mu) \\
  0 &=  \sum\limits_{n=1}^{N} x_{n} - \sum\limits_{n=1}^{N} \mu \\
  0 &=  \sum\limits_{n=1}^{N} x_{n} - N \cdot \mu \\
  N \cdot \mu &=  \sum\limits_{n=1}^{N} x_{n} \\
  \mu  &=  \frac{1}{N}\sum\limits_{n=1}^{N} x_{n} \\
\end{align}$$

Huzzah! We've reached the promised land! We now have a formula we can use to estimate one model parameter ($$\mu$$) from our data ($$\mathbf{X}$$). Let's take a second to think about what this formula means. Remember that we started with a bunch of data points:

<img src="/misc/figs/mle_1.png" align="middle" style="width: 350px;"/>


We want to take that data and make some models which we think represent that data well. In our case we are learning two Gaussian models, one for the `yellow` data and one for the `blue` data:


<img src="/misc/figs/mle_model_2.png" align="right" style="width: 350px;"/>
<img src="/misc/figs/mle_model_1.png" align="left" style="width: 350px;"/>


Yellow Model (\textsc{left}) Blue Model (\textsc{right})



To make our Gaussians fit the data as well as we can, we can do two things: (1) move the center of the curve or (2) adjust the width of the peak. Right now, with $$\mu$$ we're only talking about the placement of the center of the curve, we're not talking at all about its width.

Where is the best place to put a bell curve to cover all our data? Well, how about the center of our data! Dead-center, bull's eye, whatever you call it, we're putting our Gaussian right in the middle of it all (the mean). We're taking all our points, summing them up, and dividing by the number of data points. This is the AVERAGE, and it is (for the likelihood function) the best place to put the mean of our model.

Sure enough, if you take a look at the data, you'll see that the `yellow` data is grouped around the point `[-1,-1]` and that the `blue` data points are all clustered around `[1,1]`. Now take a look at the models we've made. You'll see that the center of the peak for the `yellow` model is somewhere near  `[-1,-1]` and that the peak of the `blue` model is around `[1,1]`.


### MLE of $$\sigma^{2}$$

Now let's tackle the second parameter of our Gaussian model, the variance $$\sigma^{2}$$!


$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= \frac{\partial}{\partial \sigma^{2}} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} )  - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &=  \frac{\partial}{\partial \sigma^{2}} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} )\Big)  + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Let's start with the product rule for the lefthand term:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &=  \frac{\partial}{\partial \sigma^{2}} \Big( -\frac{N}{2} \log ( 2\pi\sigma^{2} )\Big)  + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &=  \frac{\partial}{\partial \sigma^{2}} \big( -\frac{N}{2} \big) \cdot \log ( 2\pi\sigma^{2} ) +
  \big( -\frac{N}{2} \big) \cdot \frac{\partial}{\partial \sigma^{2}} \big( \log ( 2\pi\sigma^{2}) \big)
  + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &=  0 +
  \big( -\frac{N}{2} \big) \cdot \frac{\partial}{\partial \sigma^{2}} \big( \log ( 2\pi\sigma^{2}) \big)
  + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= -\frac{N}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \big( \log ( 2\pi\sigma^{2}) \big)
  + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Now we can use the chain rule for our term with the $$\log$$ operator, $$\frac{\partial}{\partial x}\big(f(g(x))\big) = \frac{\partial}{\partial x}f(g(x)) \cdot \frac{\partial}{\partial x}g(x)$$, with $$g(x) = 2 \pi x$$ and $$f(x) = log(x)$$.


$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \big( \log ( 2\pi\sigma^{2}) \big) + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= -\frac{N}{2} \cdot \frac{1}{2\pi\sigma^{2}} \cdot 2\pi + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= -\frac{N}{2} \cdot \frac{1}{\sigma^{2}}+ \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$

Now, using the same logic as above, we can move the derivative operator inside the summation operator:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2\sigma^{2}} + \frac{\partial}{\partial \sigma^{2}} \Big( - \frac{1}{2\sigma^{2}} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} (x_{n} - \mu)^{2} \big) \Big)\\
\end{align}$$

And again, the product rule:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} \big) + \big( - \frac{1}{2\sigma^{2}} \big) \cdot \frac{\partial}{\partial \sigma^{2}} (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} \big) + 0 \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
\end{align}$$

Now let's be careful with our exponents, since we're taking the derivative of the function with respect to a squared variable $$\sigma^2$$:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2\sigma^{2}} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2} \cdot \sigma^{-2} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
\end{align}$$

Now it's obvious that we need the product rule:
 
$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2} \cdot \sigma^{-2} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{\partial}{\partial \sigma^{2}} \big( - \frac{1}{2} \big) \cdot \sigma^{-2} + \big( - \frac{1}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \sigma^{-2} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( 0 + \big( - \frac{1}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \sigma^{-2} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( - \frac{1}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \sigma^{-2} \cdot (x_{n} - \mu)^{2} \big) \Big)\\
\end{align}$$

Now, we're going to use the chain rule again, by first treating $$\sigma^{-2} = (\sigma^{2})^{-1}$$, then we can see $$f(g(x))$$ with $$f(x) = x^{-1}$$ and then $$g(x) = x^2$$:

$$\begin{align}
  \frac{\partial \mathcal{LL}}{\partial \sigma^{2}} &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( - \frac{1}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \sigma^{-2} \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( - \frac{1}{2} \cdot \frac{\partial}{\partial \sigma^{2}} \big( (\sigma^{2})^{-1} \big) \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( - \frac{1}{2} \cdot -1 \cdot (\sigma^2)^{-2} \cdot 1 \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{1}{2} \cdot (\sigma^2)^{-2} \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \sum\limits_{n=1}^{N} \Big( \frac{1}{2\sigma^4} \cdot (x_{n} - \mu)^{2} \big) \Big)\\
  &= -\frac{N}{2\sigma^{2}} + \frac{1}{2\sigma^4} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2}\\
  &= \frac{1}{2\sigma^2} \Big( -N + \frac{1}{\sigma^2} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
\end{align}$$


Huzzah! We've gotten our partial derivative for $$\mathcal{LL}$$ with respect to $$\sigma^2$$ as simplified as we can. Now let's find the best $$\sigma^2$$ for our data by setting the equation equal to zero and solving for $$\sigma^2$$.

$$\begin{align}
  0 &= \frac{1}{2\sigma^2} \Big( -N + \frac{1}{\sigma^2} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2} \Big)\\
  0 &= -N + \frac{1}{\sigma^2} \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2}\\
  N\sigma^2 &= \sum\limits_{n=1}^{N} (x_{n} - \mu)^{2}\\
  \sigma^2 &= \frac{1}{N}\sum\limits_{n=1}^{N} (x_{n} - \mu)^{2}\\
\end{align}$$


