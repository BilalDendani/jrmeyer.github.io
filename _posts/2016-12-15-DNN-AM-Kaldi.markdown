---
layout: post
title:  "Deep Neural Net Acoustic Modeling with Kaldi"
date:   2016-12-15
categories: kaldi
comments: True
---

<img src="/misc/kaldi_text_and_logo.png" align="right" alt="logo" style="width: 400px;"/>

<br/>
<br/>
<br/>
<br/>

========================
<br/> 
THIS POST IS IN PROGRESS
<br/>
========================


If you want to take a step back and learn about Kaldi in general, I have posts on [how to install Kaldi][kaldi-install] or some [miscellaneous Kaldi notes][kaldi-notes] which contain some documentation.

## Introduction

I've you reading this, you've probably already trained a standard GMM-HMM acoustic model. 

If you haven't already, you should probably do that first, because if you're planning on following the standard Kaldi scripts for DNN training, you need to train a GMM-HMM first.

Ih you have already trained a GMM-HMM model, you should know right off-the-bat that training a DNN acoustic model does not start the same way as in training a GMM acoustic model.

In the simplest case with GMM-HMM training, we train a monophone model from a flat start (i.e. from no previous phoneme-to-audio alignments). That means we can begin training a new model with just utterance-level transcriptions. 

In DNN training, on the otherhand, we typically start training not immediately from utterance-level transcriptions, but from the labeled frames (phoneme-to-audio alignements) which were generated by an GMM-HMM system. This means, your DNN will be greatly affected by the quality of the GMM-HMM you previously trained. A bad GMM-HMM will give you bad alignments, and if you train a DNN with bad alignments, you will have a bad DNN at the end of the day, no matter how many epochs you run, what kind of cost function you use, or how clever your learning rate is.

A neural net at the end of the day is a tool for classification. We want to be able to take some new features (i.e. audio features) and assign a class to those features (i.e. a phoneme label). So, our acoustic model DNN will have input nodes which correspond to the dimensions of our audio features (think, for example, 39 input nodes for 39 MFCCs), and output nodes which correspond to senome labels (think, 900 output nodes for 900 context dependent triphones (decision tree leaves)). As you can see, two key parts of the DNN acoustic model (input layer and output layer) are modeled after the features used to train a GMM-HMM and the decision tree created by that GMM-HMM. The dimensions of the hidden layers are up to the researcher / developer.

Once we have a DNN which has the correct number of dimensions for input and output nodes, we can take our phoneme-to-audio alignments and train our neural net. The audio feature frames are fed into the input layer, the net will assign a phoneme label to a frame, and since we already have the gold-standard label (i.e. phoneme label from our GMM-HMM alignments) for any given frame, we compare what the neural net predicted and what the real phoneme was. Using some loss function and backpropigation, we iterate over all of our training frames to adjust the weights and biases of our net.

Note that unlike GMM-HMM training where in the EM algorithm we realign our transcriptions to our audio frames, with DNN training we don't have to do that. We can, but it's not necessary.

At the end, we hopefully end up with a DNN which will assign the correct phoneme label to a new, unknown audio frame.

{% highlight bash %}
{% endhighlight %}


## Training a Deep Neural Net

### README

When I start learning a new method, I begin with the most basic, well-documented code I can find. 

So, when approaching Kaldi's DNN code, I chose to start with the **nnet2** code, because even though **nnet3** is newer, the scripts for **nnet2** have been used more, reviewed more, and there's more documentation. 

Then, I chose to start with the **run_nnet2_baseline.sh** script from the Wall Street Journal **egs** directory. This script is located at **kaldi/egs/wsj/s5/local/online/run_nnet2_baseline.sh**. This is, as far as I can gather, the simplest DNN training script for Kaldi at the present moment.

### First Things First: train a GMM system and generate alignments

I'm not going to go into detail on how to train a GMM system. 

However, before you can run **run_nnet2_baseline.sh** and start training your DNN, you will need the following directories, all generated as part of normal GMM-HMM training in Kaldi: 

1. a training data dir (as generated by a run.sh script in a s5 directory)
2. a language dir (which has information on your phones, decision tree, etc)
3. an alignment dir (generated by something like **align_si.sh**).
4. a feature dir (for example MFCCs; with many Kaldi **run.sh** scripts, this is **s5/mfcc/**)

Here's the specific files you need from these four dirs:


{% highlight bash %}
## DEPENDENCIES FROM GMM-HMM SYSTEM ##

# DATA DIR FILES
$data_dir/feats.scp
$data_dir/cmvn.scp                 # assuming you did CMVN transform in GMM training
$data_dir/utt2spk 
$data_dir/utt2dur
$data_dir/splitJOBN                # where JOB is the total number of JOBs, there's only one dir here (eg. split4)
                   /JOB            # one dir for each JOB, up to JOBN
                        /feats.scp
                        /cmvn.scp
                        /utt2spk

# LANGUAGE DIR FILES
$lang_dir/topo
$lang_dir/oov.int
$lang_dir/phones/silence.csl

# ALIGN DIR FILES
$ali_dir/ali.JOB.gz                     # for as many JOBs as you ran
$ali_dir/final.mdl
$ali_dir/tree
$ali_dir/num_jobs

# MFCC DIR FILES
$mfcc_dir/raw_mfcc_train.JOB.{ark,scp}  # for as many JOBs as you ran
{% endhighlight %}


Because I like visuals and beating dead horses, here's the tree structure for the dirs and files you need from your GMM:


For the **data** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree train/
train/
├── cmvn.scp
├── feats.scp
├── split4
│   ├── 1
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 2
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 3
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   └── 4
│       ├── cmvn.scp
│       ├── feats.scp
│       └── utt2spk
├── utt2dur
└── utt2spk

5 directories, 16 files
{% endhighlight %}


For the **lang** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree lang/
lang/
├── oov.int
├── phones
│   └── silence.csl
└── topo

1 directory, 3 files
{% endhighlight %}


For the **align** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree triphones_lda_mllt_sat_aligned/
triphones_lda_mllt_sat_aligned/
├── ali.1.gz
├── ali.2.gz
├── ali.3.gz
├── ali.4.gz
├── final.mat
├── final.mdl
├── num_jobs
└── tree

0 directories, 8 files
{% endhighlight %}


For the **mfcc** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model$ tree mfcc/
mfcc/
├── raw_mfcc_train.1.ark
├── raw_mfcc_train.1.scp
├── raw_mfcc_train.2.ark
├── raw_mfcc_train.2.scp
├── raw_mfcc_train.3.ark
├── raw_mfcc_train.3.scp
├── raw_mfcc_train.4.ark
└── raw_mfcc_train.4.scp

0 directories, 8 files
{% endhighlight %}

### The Main Run Script: **run_nnet2_baseline.sh**

I've adjusted the this main run script (as well as it's dependencies) to work for my own data set. Also, I've deleted as much as I could along the way, trying to get to the bare bones of the standard Kaldi neural net training pipeline. The original authors of the scripts have added in lots of good, practical features for training and testing, but as of right now, I'm only concerned with the core of the training procedure. Keep this in mind.

This script is indeed the main **run.sh** script, but as you may have noticed in other **run.sh** scripts, this script mainly defines variables and then sends off the heavy lifting of data preparation, training, and testing to other, more specialized scripts. 

With this is mind, here's the beginning of my modified **run_nnet2_baseline.sh** script, which defines some variables and makes special adjustments for CPU vs. GPU training:


{% highlight bash %}
#!/bin/bash


# this is a baseline for ./run_nnet2.sh, without
# the iVectors, to see whether they make a difference.
stage=1
train_stage=-10
use_gpu=false
experiment_dir=experiment/nnet2_online/nnet_a_baseline
unknown_phone=SPOKEN_NOISE
silence_phone=SIL

. ./path.sh
. ./utils/parse_options.sh

if $use_gpu; then
    if ! cuda-compiled; then
        cat <<EOF && exit 1 
This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA 
If you want to use GPUs (and have them), go to src/, and configure and make on a machine
where "nvcc" is installed.
EOF
    fi
    parallel_opts="-l gpu=1" 
    num_threads=1
    minibatch_size=512
else
    # Use 4 nnet jobs just like run_4d_gpu.sh so the results should be
    # almost the same, but this may be a little bit slow.
    num_threads=16
    minibatch_size=128
    parallel_opts="-pe smp $num_threads" 
fi
{% endhighlight%}


Next, now that we've set our general variables, we have the main code for training the neural net:

{% highlight bash %}
if [ $stage -le 1 ]; then

    echo ""
    echo "######################"
    echo "### BEGIN TRAINING ###"
    echo "######################"

    steps/nnet2/train_pnorm_simple.sh \
        --stage $train_stage \
        --splice-width 4 \
        --feat-type raw \
        --cmvn-opts "--norm-means=false --norm-vars=false" \
        --num-threads "$num_threads" \
        --minibatch-size "$minibatch_size" \
        --parallel-opts "$parallel_opts" \
        --num-jobs-nnet 4 \
        --num-epochs 15 \
        --lda_dim 45 \
        --add-layers-period 2 \
        --num-hidden-layers 3 \
        --mix-up 2000 \
        --initial-learning-rate 0.02 \
        --final-learning-rate 0.004 \
        --pnorm-input-dim 500 \
        --pnorm-output-dim 100 \
        data/train \
        data/lang \
        experiment/triphones_lda_mllt_sat_aligned \
        $experiment_dir \
        || exit 1;

    echo ""
    echo "####################"
    echo "### END TRAINING ###"
    echo "####################"

fi

{% endhighlight %}

As you can see, the only obligatory arguments for **train_pnorm_simple.sh** are:

1. the training data
2. the language dir
3. our alignments from before
4. the name of the dir we want to save our new DNN model to. 

So far this is pretty straightforward: you feed in the right data to the script and you get back a nice shiny DNN. At this point, you could keep this **train_pnorm_simple.sh** script a black box, and not worry about what's going on inside, or you could dive into it. I'm going to take the time to go through it here for anyone who's interested, but if you aren't interested you can skip down below.

### The Main Train Script (called from main run script): **train_pnorm_simple.sh**

This script is a little hairy to go through, but more interesting because of it!

First, we have a bunch of default parameters to set:

{% highlight bash %}
#!/bin/bash

# Copyright 2012-2014  Johns Hopkins University (Author: Daniel Povey). 
#           2013  Xiaohui Zhang
#           2013  Guoguo Chen
#           2014  Vimal Manohar
# Apache 2.0.


# train_pnorm_simple.sh is a modified version of train_pnorm_fast.sh.  Like
# train_pnorm_fast.sh, it uses the `online' preconditioning, which is faster
# (especially on GPUs).  The difference is that the learning-rate schedule is
# simpler, with the learning rate exponentially decreasing during training,
# and no phase where the learning rate is constant.
# 
# Also, the final model-combination is done a bit differently: we combine models
# over typically a whole epoch, and because that would be too many iterations to
# easily be able to combine over, we arrange the iterations into groups (20
# groups by default) and average over each group.
#

# Begin configuration section.
cmd=run.pl
num_epochs=15      # Number of epochs of training;
                   # the number of iterations is worked out from this.
initial_learning_rate=0.04
final_learning_rate=0.004
bias_stddev=0.5
pnorm_input_dim=3000 
pnorm_output_dim=300
p=2
minibatch_size=128 # by default use a smallish minibatch size for neural net
                   # training; this controls instability which would otherwise
                   # be a problem with multi-threaded update. 

samples_per_iter=400000 # each iteration of training, see this many samples
                        # per job.  This option is passed to get_egs.sh
num_jobs_nnet=16   # Number of neural net jobs to run in parallel.  This option
                   # is passed to get_egs.sh.
get_egs_stage=0
online_ivector_dir=


max_models_combine=20 # The "max_models_combine" is the maximum number of models we give
  # to the final 'combine' stage, but these models will themselves be averages of
  # iteration-number ranges.

shuffle_buffer_size=5000 # This "buffer_size" variable controls randomization of the samples
                # on each iter.  You could set it to 0 or to a large value for complete
                # randomization, but this would both consume memory and cause spikes in
                # disk I/O.  Smaller is easier on disk and memory but less random.  It's
                # not a huge deal though, as samples are anyway randomized right at the start.
                # (the point of this is to get data in different minibatches on different iterations,
                # since in the preconditioning method, 2 samples in the same minibatch can
                # affect each others' gradients.

add_layers_period=2 # by default, add new layers every 2 iterations.
num_hidden_layers=3
stage=-4

io_opts="-tc 5" # for jobs with a lot of I/O, limits the number running at one time.   These don't
splice_width=4 # meaning +- 4 frames on each side for second LDA
randprune=4.0 # speeds up LDA.
alpha=4.0 # relates to preconditioning.
update_period=4 # relates to online preconditioning: says how often we update the subspace.
num_samples_history=2000 # relates to online preconditioning
max_change_per_sample=0.075
precondition_rank_in=20  # relates to online preconditioning
precondition_rank_out=80 # relates to online preconditioning

mix_up=0 # Number of components to mix up to (should be > #tree leaves, if
        # specified.)
num_threads=16
parallel_opts="--num-threads 16 --mem 1G" 
  # by default we use 16 threads; this lets the queue know.
  # note: parallel_opts doesn't automatically get adjusted if you adjust num-threads.
combine_num_threads=8
combine_parallel_opts="--num-threads 8"  # queue options for the "combine" stage.
cleanup=true
egs_dir=
lda_opts=
lda_dim=
egs_opts=
transform_dir=     # If supplied, overrides ali_dir
cmvn_opts=  # will be passed to get_lda.sh and get_egs.sh, if supplied.  
            # only relevant for "raw" features, not lda.
feat_type=  # Can be used to force "raw" features.
prior_subset_size=10000 # 10k samples per job, for computing priors.  Should be
                        # more than enough.

# End configuration section.

{% endhighlight %}

After parsing the command-line arguments, we check to make sure our important files are where they should be. All these files should have been generated during our GMM-HMM training.

{% highlight bash %}

data_dir=$1
lang_dir=$2
ali_dir=$3
exp_dir=$4


# Check some files.
for f in \
    $data_dir/feats.scp \
    $data_dir/utt2spk \
    $lang_dir/topo \
    $lang_dir/oov.int \
    $lang_dir/phones/silence.csl \
    $ali_dir/ali.1.gz \
    $ali_dir/final.mdl \
    $ali_dir/tree \
    $ali_dir/num_jobs;
    do [ ! -f $f ] && echo "$0: no such file $f" && exit 1;
done

{% endhighlight %}


Once we're sure all the important files are in place, we extract some variables from the information in those files:


{% highlight bash %}
# Set Number of leaves.
num_leaves=`tree-info $ali_dir/tree 2>/dev/null | grep num-pdfs | awk '{print $2}'` \
    || exit 1;
[ -z $num_leaves ] \
    && echo "\$num_leaves is unset" \
    && exit 1;
[ "$num_leaves" -eq "0" ] \
    && echo "\$num_leaves is 0" \
    && exit 1;

# number of jobs in alignment dir...
nj=`cat $ali_dir/num_jobs` \
    || exit 1;

# in this dir we'll have just one job.
sdata=$data_dir/split$nj
utils/split_data.sh $data_dir $nj

mkdir -p $exp_dir/log
echo $nj > $exp_dir/num_jobs
cp $ali_dir/tree $exp_dir

extra_opts=()
[ ! -z "$cmvn_opts" ] && extra_opts+=(--cmvn-opts "$cmvn_opts")
[ ! -z "$feat_type" ] && extra_opts+=(--feat-type $feat_type)
[ ! -z "$online_ivector_dir" ] && extra_opts+=(--online-ivector-dir $online_ivector_dir)
[ -z "$transform_dir" ] && transform_dir=$ali_dir
extra_opts+=(--transform-dir $transform_dir)
extra_opts+=(--splice-width $splice_width)
{% endhighlight %}

At this point in the script, we have defined a bunch of variables, created two files (1) **tree** (copied from the GMM-HMM) and (2) **num_jobs**, and created an empty **log** directory.


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2_online/
nnet2_online/
└── nnet_a_baseline
    ├── log
    ├── num_jobs
    └── tree

2 directories, 2 files
{% endhighlight %}

Now we move on to data preparation before training, and we start with getting the LDA feature transform (via **get_lda.sh**) we will use before we take our features as input to our DNN.

Straight from the comments in the script itself:

> This script, which will generally be called from other neural-net training
> scripts, extracts the training examples used to train the neural net (and also
> the validation examples used for diagnostics), and puts them in separate archives.
>
> As well as extracting the examples, this script will also do the LDA computation,
> if --est-lda=true (default:true)

{% highlight bash %}
if [ $stage -le -4 ]; then

    echo ""
    echo "########################"
    echo "### BEGIN get_lda.sh ###"
    echo "########################"

    steps/nnet2/get_lda.sh \
        --cmd "$cmd" \
        --lda-dim $lda_dim \
        "${extra_opts[@]}" \
        $data_dir \
        $lang_dir \
        $ali_dir \
        $exp_dir \
        || exit 1;
fi

# these files should have been written by get_lda.sh
feat_dim=$(cat $exp_dir/feat_dim) || exit 1;
ivector_dim=$(cat $exp_dir/ivector_dim) || exit 1;
lda_dim=$(cat $exp_dir/lda_dim) || exit 1;

{% endhighlight %}


This script outputs a matrix for the LDA transform, and this very matrix will come up again when we initialize the neural net as a **FixedAffineComponent**, just after our input layer with splicing. That means, once we've got our LDA transform, it will applied to all input, and the matrix will not be updated by back-propagation.

Here are the new files created by **get_lda.sh**.

{% highlight bash %}
# At this point, we have generated the following:
# josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2_online/
# nnet2_online/
# └── nnet_a_baseline
#     ├── cholesky.tpmat
#     ├── cmvn_opts
#     ├── feat_dim
#     ├── ivector_dim
#     ├── lda.acc
#     ├── lda_dim
#     ├── lda.mat
#     ├── log
#     │   ├── lda_acc.1.log
#     │   ├── lda_acc.2.log
#     │   ├── lda_acc.3.log
#     │   ├── lda_acc.4.log
#     │   ├── lda_est.log
#     │   └── lda_sum.log
#     └── within_covar.spmat

# 2 directories, 14 files
{% endhighlight %}

Now that we've estimated the LDA transform we will apply to our spliced feature frames, we will split up our data into the appropriate directories for training and validation:

{% highlight bash  %}
if [ $stage -le -3 ] && [ -z "$egs_dir" ]; then

    echo ""
    echo "########################"
    echo "### BEGIN get_egs.sh ###"
    echo "########################"

    steps/nnet2/get_egs.sh \
        --cmd "$cmd" \
        "${extra_opts[@]}" \
        --samples-per-iter $samples_per_iter \
        --num-jobs-nnet $num_jobs_nnet \
        --io-opts "$io_opts" \
        $data_dir \
        $ali_dir \
        $exp_dir \
        || exit 1;
fi

if [ -z $egs_dir ]; then
    egs_dir=$exp_dir/egs
fi

{% endhighlight %}

After we run the **get_egs.sh** script, we find that we have generated 27 new files and one new **egs** dir.

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2_online/
nnet2_online/
└── nnet_a_baseline
    ├── egs
    │   ├── combine.egs
    │   ├── egs.1.0.ark
    │   ├── egs.2.0.ark
    │   ├── egs.3.0.ark
    │   ├── egs.4.0.ark
    │   ├── iters_per_epoch
    │   ├── num_jobs_nnet
    │   ├── samples_per_iter
    │   ├── train_diagnostic.egs
    │   └── valid_diagnostic.egs
    ├── log
    │   ├── create_train_subset_combine.log
    │   ├── create_train_subset_diagnostic.log
    │   ├── create_train_subset.log
    │   ├── create_valid_subset_combine.log
    │   ├── create_valid_subset_diagnostic.log
    │   ├── create_valid_subset.log
    │   ├── get_egs.1.log
    │   ├── get_egs.2.log
    │   ├── get_egs.3.log
    │   ├── get_egs.4.log
    │   ├── shuffle.0.1.log
    │   ├── shuffle.0.2.log
    │   ├── shuffle.0.3.log
    │   └── shuffle.0.4.log
    ├── num_frames
    ├── train_subset_uttlist
    └── valid_uttlist

3 directories, 27 files
{% endhighlight %}

Now that we have the training examples (phone-to-frame alignments) sorted and in the correct format, we go on to initialize our neural net.

Similar to our **topo** configuration file we used in GMM-HMM training, we have to define the dimentions and nature of our neural net before we can initialize it. These definitions are stored in a configuration file called **nnet.config**, and it gets defined and saved to disk as a normal part of the **train_pnorm_simple.sh** script.


{% highlight bash %}
    cat >$exp_dir/nnet.config <<EOF
SpliceComponent input-dim=$tot_input_dim left-context=$splice_width right-context=$splice_width const-component-dim=$ivector_dim
FixedAffineComponent matrix=$lda_mat
AffineComponentPreconditionedOnline input-dim=$lda_dim output-dim=$pnorm_input_dim $online_preconditioning_opts learning-rate=$initial_learning_rate param-stddev=$stddev bias-stddev=$bias_stddev
PnormComponent input-dim=$pnorm_input_dim output-dim=$pnorm_output_dim p=$p
NormalizeComponent dim=$pnorm_output_dim
AffineComponentPreconditionedOnline input-dim=$pnorm_output_dim output-dim=$num_leaves $online_preconditioning_opts learning-rate=$initial_learning_rate param-stddev=0 bias-stddev=0
SoftmaxComponent dim=$num_leaves
EOF
{% endhighlight %}

*The below descriptions are taken almost verbatim from the official Kaldi [nnet2 docs][nnet2-docs].*

The **SpliceComponent** defines the size of the window of feature-frame-splicing to perform. 

The **FixedAffineComponent** is our LDA-like transform created by **get_lda.sh**.

The **AffineComponentPreconditionedOnline** is like a standard Wx+b affine transform found in neural nets, but the online precoditioning adds a special learning-rate feature.

The **PnormComponent** is the nonlinearity; for a more conventional neural network this would be **TanhComponent** instead. 

The **NormalizeComponent** is something we add to stabilize the training of p-norm networks. 

The **SoftmaxComponent** is the final nonlinearity that produces properly normalized probabilities at the output.


This initial DNN configuration contains one and only one hidden layer. 

That is, there are seven *components* and three layers. It follows that there are only two weight matrices and two bias vectors. If we look back at the **nnet.config** file definition, there are indeed only two components which are updatable, both are of the form **AffineComponentPreconditionedOnline**.

Next in the **train_pnorm_simple.sh** script, we create another, similar configuration file which defines the kind of hidden layers we will be adding to our net during training:


{% highlight bash %}
    cat >$exp_dir/hidden.config <<EOF
AffineComponentPreconditionedOnline input-dim=$pnorm_output_dim output-dim=$pnorm_input_dim $online_preconditioning_opts learning-rate=$initial_learning_rate param-stddev=$stddev bias-stddev=$bias_stddev
PnormComponent input-dim=$pnorm_input_dim output-dim=$pnorm_output_dim p=$p
NormalizeComponent dim=$pnorm_output_dim
EOF
{% endhighlight %}

Again, we find here an affine transform, followed by a non-linearity, and finally a normalization. Nothing too crazy.

Now, we can put our decision tree, HMM topology file, and **nnet.config** file to work and initialize our first neural net, aka **0.mdl**:

{% highlight bash %}
    $cmd $exp_dir/log/nnet_init.log \
        nnet-am-init \
            $ali_dir/tree \
            $lang_dir/topo \
            "nnet-init $exp_dir/nnet.config -|" \
            $exp_dir/0.mdl \
            || exit 1;
{% endhighlight %}


Since the last time we did a check in with what files we created, we've only created four new files:

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2_online/
nnet2_online/
└── nnet_a_baseline
    ├── 0.mdl
    ├── hidden.config
    ├── log
    │   └── nnet_init.log
    └── nnet.config

2 directories, 4 files
{% endhighlight %}

Also, we can take a look at our untrained model and get some info with **nnet-am-info.cc**:

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment/nnet2_online/nnet_a_baseline$ ../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
num-components 7
num-updatable-components 2
left-context 2
right-context 2
input-dim 13
output-dim 1759
parameter-dim 51484
component 0 : SpliceComponent, input-dim=13, output-dim=65, context=-2 -1 0 1 2 
component 1 : FixedAffineComponent, input-dim=65, output-dim=45, linear-params-stddev=0.0209558, bias-params-stddev=2.4487
component 2 : AffineComponentPreconditionedOnline, input-dim=45, output-dim=125, linear-params-stddev=0.0901615, bias-params-stddev=0.484466, learning-rate=0.02, rank-in=20, rank-out=80, num_samples_history=2000, update_period=4, alpha=4, max-change-per-sample=0.075
component 3 : PnormComponent, input-dim = 125, output-dim = 25, p = 2
component 4 : NormalizeComponent, input-dim=25, output-dim=25
component 5 : AffineComponentPreconditionedOnline, input-dim=25, output-dim=1759, linear-params-stddev=0, bias-params-stddev=0, learning-rate=0.02, rank-in=20, rank-out=80, num_samples_history=2000, update_period=4, alpha=4, max-change-per-sample=0.075
component 6 : SoftmaxComponent, input-dim=1759, output-dim=1759
LOG (nnet-am-info:main():nnet-am-info.cc:76) Printed info about 0.mdl
prior dimension: 0
{% endhighlight %}


Now that we have an initialized model and labeled training examples to go along, we can train the transitions of the HMMs in our DNN-HMM acoustic model. In GMM-HMM training the transitions get updated during EM training, but since we are not doing any realignment for DNN training, the initial transitional probabilities will do just fine.

{% highlight bash %}
if [ $stage -le -1 ]; then

    echo "### TRAIN TRANSITION PROBS AND SET PRIORS ###"

    $cmd $exp_dir/log/train_trans.log \
        nnet-train-transitions \
            $exp_dir/0.mdl \
            "ark:gunzip -c $ali_dir/ali.*.gz|" \
            $exp_dir/0.mdl \
            || exit 1;
fi

{% endhighlight %}

Since we are seeding with the original **0.mdl** and resaving it as the same name **0.mdl**, the only file produced by this command is the log file **train_trans.log**.

Taking another quote from the offical nnet2 docs with regards to **nnet-train-transitions.cc**:

>This computes the transition probabilities that will be used in the HMMs in decoding (which has nothing to do with the neural net itself), and also computes the prior probabilities of the "targets" (the several thousand context-dependent states). Later, when we do decoding, we will divide the posteriors computed by the network by these priors to get "pseudo-likelihoods"; these are more compatible with the HMM framework than raw posteriors.

These priors are stored back in the model, and we can see that they exist by again getting info about our neural net with **nnet-am-info.cc**

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment/nnet2_online/nnet_a_baseline$ ../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
num-components 7
num-updatable-components 2
left-context 2
right-context 2
input-dim 13
output-dim 1759
parameter-dim 51484
component 0 : SpliceComponent, input-dim=13, output-dim=65, context=-2 -1 0 1 2 
component 1 : FixedAffineComponent, input-dim=65, output-dim=45, linear-params-stddev=0.0209558, bias-params-stddev=2.4487
component 2 : AffineComponentPreconditionedOnline, input-dim=45, output-dim=125, linear-params-stddev=0.0901615, bias-params-stddev=0.484466, learning-rate=0.02, rank-in=20, rank-out=80, num_samples_history=2000, update_period=4, alpha=4, max-change-per-sample=0.075
component 3 : PnormComponent, input-dim = 125, output-dim = 25, p = 2
component 4 : NormalizeComponent, input-dim=25, output-dim=25
component 5 : AffineComponentPreconditionedOnline, input-dim=25, output-dim=1759, linear-params-stddev=0, bias-params-stddev=0, learning-rate=0.02, rank-in=20, rank-out=80, num_samples_history=2000, update_period=4, alpha=4, max-change-per-sample=0.075
component 6 : SoftmaxComponent, input-dim=1759, output-dim=1759
prior dimension: 1759, prior sum: 1, prior min: 1.68406e-05
{% endhighlight %}

Now we move on to the main training loop, which will actually update our parameters via backpropagation.

Here's the most important code snippet from this loop:


{% highlight bash %}
        $cmd $parallel_opts JOB=1:$num_jobs_nnet $exp_dir/log/train.$x.JOB.log \
            nnet-shuffle-egs \
                --buffer-size=$shuffle_buffer_size \
                --srand=$x \
                ark:$cur_egs_dir/egs.JOB.$[$x%$iters_per_epoch].ark \
                ark:- \| \
            nnet-train-parallel \
                --num-threads=$num_threads \
                --minibatch-size=$this_minibatch_size \
                --srand=$x "$mdl" \
                ark:- \
                $exp_dir/$[$x+1].JOB.mdl \
            || exit 1; 
{% endhighlight %}

Copy-and-pasting from the comments of the **nnet-train-parallel.cc** source code:

>Train the neural network parameters with backprop and stochastic
>gradient descent using minibatches.  As nnet-train-simple, but
>uses multiple threads in a Hogwild type of update (for CPU, not GPU).

## Conclusion

I hope this was helpful!

If you have any feedback or questions, don't hesitate to leave a comment!


## Relevant Papers

### Maas et al. (2013) [*Building DNN Acoustic Models for Large Vocabulary Speech Recognition*][maas-2014]

From the abstract:

> Building  neural  network  acoustic  models  requires  several
> design decisions including network architecture, size, and train-
> ing loss function. This paper offers an empirical investigation on
> which aspects of DNN acoustic model design are most important
> for speech recognition system performance. We report DNN clas-
> sifier performance and final speech recognizer word error rates,
> and  compare  DNNs  using  several  metrics  to  quantify  factors
> influencing differences in task performance.


Co-authors include Dan Jurafsky and Andrew Ng, among others. This is a longer paper (22 pages) and is a very thorough 

[kaldi-install]: http://jrmeyer.github.io/kaldi/2016/01/26/Installing-Kaldi.html
[kaldi-notes]: http://jrmeyer.github.io/kaldi/2016/02/01/Kaldi-notes.html
[maas-2014]: https://arxiv.org/pdf/1406.7806.pdf
[nnet2-docs]: http://kaldi-asr.org/doc/dnn2.html