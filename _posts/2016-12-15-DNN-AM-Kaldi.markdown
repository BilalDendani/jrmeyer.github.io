---
layout: post
title:  "Deep Neural Net Acoustic Modeling with Kaldi"
date:   2016-12-15
categories: kaldi
comments: True
---

<img src="/misc/kaldi_text_and_logo.png" align="right" alt="logo" style="width: 400px;"/>

<br/>
<br/>
<br/>
<br/>

========================
<br/> 
THIS POST IS IN PROGRESS
<br/>
========================


If you want to take a step back and learn about Kaldi in general, I have posts on [how to install Kaldi][kaldi-install] or some [miscellaneous Kaldi notes][kaldi-notes] which contain some documentation.

## Introduction

I've you reading this, you've probably already trained a standard GMM-HMM acoustic model. If you haven't already, you should probably do that first, because if you're planning on following the standard Kaldi scripts for DNN training, you need to train a GMM-HMM first.

IF you've already trained a GMM-HMM model, you should know right off-the-bat that Training a DNN acoustic model does not start the same way as in training a GMM acoustic model.

In the simplest case with GMM-HMM training, we train a monophone model from a flat start (i.e. from no previous phoneme-to-audio alignments). That means we can begin training a new model with just utterance-level transcriptions. 

In DNN training, on the otherhand, we typically start training not immediately from utterance-level transcriptions, but from the labeled frames (phoneme-to-audio alignements) which were generated by an HMM-GMM system. This means, your DNN will be greatly affected by the quality of the GMM-HMM you previously trained. A bad GMM-HMM will give you bad alignments, and if you train a DNN with bad alignments, you will have a bad DNN at the end of the day, no matter how many epochs you run, what kind of cost function you use, or how clever your learning rate is.

A neural net at the end of the day is a tool for classification. We want to be able to take some new features (i.e. audio features) and assign a class to those features (i.e. a phoneme label). So, our acoustic model DNN will have input nodes which correspond to the dimensions of our audio features (think, for example, 39 input nodes for 39 MFCCs), and output nodes which correspond to senome labels (think, 900 output nodes for 900 context dependent triphones (decision tree leaves)). As you can see, two key parts of the DNN acoustic model (input layer and output layer) are modeled after the features used to train a HMM-GMM and the decision tree created by that GMM-HMM. The dimensions of the hidden layers are up to the researcher / developer.

Once we have a DNN which has the correct number of dimensions for input and output nodes, we can take our phoneme-to-audio alignments and train our neural net. The audio feature frames are fed into the input layer, the net will assign a phoneme label to a frame, and since we already have the gold-standard label (i.e. phoneme label from our GMM-HMM alignments) for any given frame, we compare what the neural net predicted and what the real phoneme was. Using some loss function and backpropigation, we iterate over all of our training frames to adjust the weights and biases of our net.

Note that unlike GMM-HMM training where in the EM algorithm we realign our transcriptions to our audio frames, with DNN training we don't have to do that. We can, but it's not necessary.

At the end, we hopefully end up with a DNN which will assign the correct phoneme label to a new, unknown audio frame.

{% highlight bash %}
{% endhighlight %}


## Training a Deep Neural Net

### README

I always try to start learning a new method with the most basic code I can find. To begin with, when approaching Kaldi's DNN code, I chose to start with the **nnet2** code, because even though **nnet3** is newer, the scripts for **nnet2** have been used more, reviewed more, and there's more documentation. Then, I chose to start with the **run_nnet2_baseline.sh** script from the Wall Street Journal **egs** directory. This script is located at **kaldi/egs/wsj/s5/local/online/run_nnet2_baseline.sh**.

### First Things First: train a GMM system and generate alignments

I'm not going to go into detail on how to train a GMM system. 

However, before you can run **run_nnet2_baseline.sh** start training your DNN, you will need the following files, all generated as part of normal GMM-HMM training in Kaldi: 

1. a training data dir (as generated by a run.sh script in a s5 directory)
2. a language dir (which has information on your phones, decision tree, etc)
3. an alignment dir (generated by something like **align_si.sh**).
4. a feature dir (for example MFCCs; with many Kaldi **run.sh** scripts, this is **s5/mfcc/**)

Here's the specific files you need from these three dirs:


{% highlight bash %}
## DEPENDENCIES FROM GMM-HMM SYSTEM ##

# DATA DIR FILES
$data_dir/feats.scp
$data_dir/cmvn.scp                 # assuming you did CMVN transform in GMM training
$data_dir/utt2spk 
$data_dir/utt2dur
$data_dir/splitJOBN                # where JOB is the total number of JOBs, there's only one dir here (eg. split4)
                   /JOB            # one dir for each JOB, up to JOBN
                        /feats.scp
                        /cmvn.scp
                        /utt2spk

# LANGUAGE DIR FILES
$lang_dir/topo
$lang_dir/oov.int
$lang_dir/phones/silence.csl

# ALIGN DIR FILES
$ali_dir/ali.JOB.gz                     # for as many JOBs as you ran
$ali_dir/final.mdl
$ali_dir/tree
$ali_dir/num_jobs

# MFCC DIR FILES
$mfcc_dir/raw_mfcc_train.JOB.{ark,scp}  # for as many JOBs as you ran
{% endhighlight %}

Because I like visuals and beating dead horses, here's the tree structure for the files you need from your GMM:


For the **data** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree train/
train/
├── cmvn.scp
├── feats.scp
├── split4
│   ├── 1
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 2
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 3
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   └── 4
│       ├── cmvn.scp
│       ├── feats.scp
│       └── utt2spk
├── utt2dur
└── utt2spk

5 directories, 16 files
{% endhighlight %}


For the **lang** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree lang/
lang/
├── oov.int
├── phones
│   └── silence.csl
└── topo

1 directory, 3 files
{% endhighlight %}


For the **align** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree triphones_lda_mllt_sat_aligned/
triphones_lda_mllt_sat_aligned/
├── ali.1.gz
├── ali.2.gz
├── ali.3.gz
├── ali.4.gz
├── final.mat
├── final.mdl
├── num_jobs
└── tree

0 directories, 8 files
{% endhighlight %}


For the **mfcc** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model$ tree mfcc/
mfcc/
├── raw_mfcc_train.1.ark
├── raw_mfcc_train.1.scp
├── raw_mfcc_train.2.ark
├── raw_mfcc_train.2.scp
├── raw_mfcc_train.3.ark
├── raw_mfcc_train.3.scp
├── raw_mfcc_train.4.ark
└── raw_mfcc_train.4.scp

0 directories, 8 files
{% endhighlight %}

### The Main Run Script: **run_nnet2_baseline.sh**

I've adjusted the this main run script (as well as it's dependencies) to work for my own data set. Also, I've deleted as much as I could along the way, trying to get to the bare bones of the standard Kaldi neural net training pipeline. The original authors of the scripts have added in lots of good, practical features for training and testing, but as of right now, I'm only concerned with the core of the training procedure. Keep this in mind - I might have mistakenly deleted a part of training that's very important, so if you spot something strange please let me know.

This script is indeed the main run.sh script, but as you may have noticed in other run.sh scripts, this script mainly defines variables and then sends off the heavy lifting of data preparation, training, and testing to other, more specialized scripts. 

With this is mind, here's the beginning of my modified run script, which defines some variables and makes special adjustments for CPU vs. GPU training:


{% highlight bash %}
#!/bin/bash


# this is a baseline for ./run_nnet2.sh, without
# the iVectors, to see whether they make a difference.
stage=1
train_stage=-10
use_gpu=false
experiment_dir=experiment/nnet2_online/nnet_a_baseline
unknown_phone=SPOKEN_NOISE
silence_phone=SIL

. ./path.sh
. ./utils/parse_options.sh

if $use_gpu; then
    if ! cuda-compiled; then
        cat <<EOF && exit 1 
This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA 
If you want to use GPUs (and have them), go to src/, and configure and make on a machine
where "nvcc" is installed.
EOF
    fi
    parallel_opts="-l gpu=1" 
    num_threads=1
    minibatch_size=512
else
    # Use 4 nnet jobs just like run_4d_gpu.sh so the results should be
    # almost the same, but this may be a little bit slow.
    num_threads=16
    minibatch_size=128
    parallel_opts="-pe smp $num_threads" 
fi
{% endhighlight%}


Next, now that we've set our general variables, we have the main code for training the neural net:

{% highlight bash %}
if [ $stage -le 1 ]; then

    echo ""
    echo "######################"
    echo "### BEGIN TRAINING ###"
    echo "######################"

    steps/nnet2/train_pnorm_simple.sh \
        --stage $train_stage \
        --splice-width 4 \
        --feat-type raw \
        --cmvn-opts "--norm-means=false --norm-vars=false" \
        --num-threads "$num_threads" \
        --minibatch-size "$minibatch_size" \
        --parallel-opts "$parallel_opts" \
        --num-jobs-nnet 4 \
        --num-epochs 15 \
        --add-layers-period 2 \
        --num-hidden-layers 3 \
        --mix-up 2000 \
        --initial-learning-rate 0.02 \
        --final-learning-rate 0.004 \
        --pnorm-input-dim 500 \
        --pnorm-output-dim 100 \
        data/train \
        data/lang \
        experiment/triphones_lda_mllt_sat_aligned \
        $experiment_dir \
        || exit 1;

    echo ""
    echo "####################"
    echo "### END TRAINING ###"
    echo "####################"

fi

{% endhighlight %}

As you can see, the only obligatory arguments for **train_pnorm_simple.sh** are (1) the training data, (2) the language dir, (3) our alignments from before, and (4) the name of the dir we want to save our new DNN model to. 

So far this is pretty straightforward: you feed in the right data to the script and you get back a nice shiny DNN. At this point, you could keep this **train_pnorm_simple.sh** script a black box, and not worry about what's going on inside, or you could dive into it. I'm going to take the time to go through it here for anyone who's interested, but if you aren't interested you can skip down below.

### The Main Train Script (called from main run script): **train_pnorm_simple.sh**

This script is a little hairy to go through, but more interesting because of it!

First, we have a bunch of default parameters to set:

{% highlight bash %}
#!/bin/bash

# Copyright 2012-2014  Johns Hopkins University (Author: Daniel Povey). 
#           2013  Xiaohui Zhang
#           2013  Guoguo Chen
#           2014  Vimal Manohar
# Apache 2.0.


# train_pnorm_simple.sh is a modified version of train_pnorm_fast.sh.  Like
# train_pnorm_fast.sh, it uses the `online' preconditioning, which is faster
# (especially on GPUs).  The difference is that the learning-rate schedule is
# simpler, with the learning rate exponentially decreasing during training,
# and no phase where the learning rate is constant.
# 
# Also, the final model-combination is done a bit differently: we combine models
# over typically a whole epoch, and because that would be too many iterations to
# easily be able to combine over, we arrange the iterations into groups (20
# groups by default) and average over each group.
#

# Begin configuration section.
cmd=run.pl
num_epochs=15      # Number of epochs of training;
                   # the number of iterations is worked out from this.
initial_learning_rate=0.04
final_learning_rate=0.004
bias_stddev=0.5
pnorm_input_dim=3000 
pnorm_output_dim=300
p=2
minibatch_size=128 # by default use a smallish minibatch size for neural net
                   # training; this controls instability which would otherwise
                   # be a problem with multi-threaded update. 

samples_per_iter=400000 # each iteration of training, see this many samples
                        # per job.  This option is passed to get_egs.sh
num_jobs_nnet=16   # Number of neural net jobs to run in parallel.  This option
                   # is passed to get_egs.sh.
get_egs_stage=0
online_ivector_dir=


max_models_combine=20 # The "max_models_combine" is the maximum number of models we give
  # to the final 'combine' stage, but these models will themselves be averages of
  # iteration-number ranges.

shuffle_buffer_size=5000 # This "buffer_size" variable controls randomization of the samples
                # on each iter.  You could set it to 0 or to a large value for complete
                # randomization, but this would both consume memory and cause spikes in
                # disk I/O.  Smaller is easier on disk and memory but less random.  It's
                # not a huge deal though, as samples are anyway randomized right at the start.
                # (the point of this is to get data in different minibatches on different iterations,
                # since in the preconditioning method, 2 samples in the same minibatch can
                # affect each others' gradients.

add_layers_period=2 # by default, add new layers every 2 iterations.
num_hidden_layers=3
stage=-4

io_opts="-tc 5" # for jobs with a lot of I/O, limits the number running at one time.   These don't
splice_width=4 # meaning +- 4 frames on each side for second LDA
randprune=4.0 # speeds up LDA.
alpha=4.0 # relates to preconditioning.
update_period=4 # relates to online preconditioning: says how often we update the subspace.
num_samples_history=2000 # relates to online preconditioning
max_change_per_sample=0.075
precondition_rank_in=20  # relates to online preconditioning
precondition_rank_out=80 # relates to online preconditioning

mix_up=0 # Number of components to mix up to (should be > #tree leaves, if
        # specified.)
num_threads=16
parallel_opts="--num-threads 16 --mem 1G" 
  # by default we use 16 threads; this lets the queue know.
  # note: parallel_opts doesn't automatically get adjusted if you adjust num-threads.
combine_num_threads=8
combine_parallel_opts="--num-threads 8"  # queue options for the "combine" stage.
cleanup=true
egs_dir=
lda_opts=
lda_dim=
egs_opts=
transform_dir=     # If supplied, overrides ali_dir
cmvn_opts=  # will be passed to get_lda.sh and get_egs.sh, if supplied.  
            # only relevant for "raw" features, not lda.
feat_type=  # Can be used to force "raw" features.
prior_subset_size=10000 # 10k samples per job, for computing priors.  Should be
                        # more than enough.

# End configuration section.

{% endhighlight %}

After parsing the command-line arguments, we check to make sure our important files are where they should be. All these files should have been generated during our GMM-HMM training.

{% highlight bash %}

data_dir=$1
lang_dir=$2
ali_dir=$3
exp_dir=$4


# Check some files.
for f in \
    $data_dir/feats.scp \
    $data_dir/utt2spk \
    $lang_dir/topo \
    $lang_dir/oov.int \
    $lang_dir/phones/silence.csl \
    $ali_dir/ali.1.gz \
    $ali_dir/final.mdl \
    $ali_dir/tree \
    $ali_dir/num_jobs;
    do [ ! -f $f ] && echo "$0: no such file $f" && exit 1;
done

{% endhighlight %}


Once we're sure all the important files are in place, we extract some variables from the information in those files:


{% highlight bash %}
# Set Number of leaves.
num_leaves=`tree-info $ali_dir/tree 2>/dev/null | grep num-pdfs | awk '{print $2}'` \
    || exit 1;
[ -z $num_leaves ] \
    && echo "\$num_leaves is unset" \
    && exit 1;
[ "$num_leaves" -eq "0" ] \
    && echo "\$num_leaves is 0" \
    && exit 1;

# number of jobs in alignment dir...
nj=`cat $ali_dir/num_jobs` \
    || exit 1;

# in this dir we'll have just one job.
sdata=$data_dir/split$nj
utils/split_data.sh $data_dir $nj

mkdir -p $exp_dir/log
echo $nj > $exp_dir/num_jobs
cp $ali_dir/tree $exp_dir

extra_opts=()
[ ! -z "$cmvn_opts" ] && extra_opts+=(--cmvn-opts "$cmvn_opts")
[ ! -z "$feat_type" ] && extra_opts+=(--feat-type $feat_type)
[ ! -z "$online_ivector_dir" ] && extra_opts+=(--online-ivector-dir $online_ivector_dir)
[ -z "$transform_dir" ] && transform_dir=$ali_dir
extra_opts+=(--transform-dir $transform_dir)
extra_opts+=(--splice-width $splice_width)
{% endhighlight %}

Now we move on to data preparation before training, and we start with getting the LDA feature transform we will use before we take our features as input to our DNN.

Straight from the comments in the script itself:

> This script, which will generally be called from other neural-net training
> scripts, extracts the training examples used to train the neural net (and also
> the validation examples used for diagnostics), and puts them in separate archives.
>
> As well as extracting the examples, this script will also do the LDA computation,
> if --est-lda=true (default:true)

{% highlight bash %}
if [ $stage -le -4 ]; then

    echo ""
    echo "########################"
    echo "### BEGIN get_lda.sh ###"
    echo "########################"

    steps/nnet2/get_lda.sh \
        $lda_opts \
        "${extra_opts[@]}" \
        --cmd "$cmd" \
        $data_dir \
        $lang_dir \
        $ali_dir \
        $exp_dir \
        || exit 1;
fi
{% endhighlight %}


This script outputs a matrix for the LDA transform, and this very matrix will come up again when we initialize the neural net as a **FixedAffineComponent**, just after our input layer with splicing. That means, once we've got our LDA transform, it will applied to all input, and the matrix will not be updated by back-propagation.



## Conclusion

I hope this was helpful!

If you have any feedback or questions, don't hesitate to leave a comment!


## Relevant Papers

### Maas et al. (2013) [*Building DNN Acoustic Models for Large Vocabulary Speech Recognition*][maas-2014]

From the abstract:

> Building  neural  network  acoustic  models  requires  several
> design decisions including network architecture, size, and train-
> ing loss function. This paper offers an empirical investigation on
> which aspects of DNN acoustic model design are most important
> for speech recognition system performance. We report DNN clas-
> sifier performance and final speech recognizer word error rates,
> and  compare  DNNs  using  several  metrics  to  quantify  factors
> influencing differences in task performance.


Co-authors include Dan Jurafsky and Andrew Ng, among others. This is a longer paper (22 pages) and is a very thorough 

[kaldi-install]: http://jrmeyer.github.io/kaldi/2016/01/26/Installing-Kaldi.html
[kaldi-notes]: http://jrmeyer.github.io/kaldi/2016/02/01/Kaldi-notes.html
[maas-2014]: https://arxiv.org/pdf/1406.7806.pdf