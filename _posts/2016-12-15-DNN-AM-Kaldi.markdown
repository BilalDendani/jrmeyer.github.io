---
layout: post
title:  "Deep Neural Net Acoustic Modeling with Kaldi"
date:   2016-12-15
categories: kaldi
comments: True
---

<img src="/misc/kaldi_text_and_logo.png" align="right" alt="logo" style="width: 400px;"/>

<br/>
<br/>
<br/>
<br/>

========================
<br/> 
THIS POST IS IN PROGRESS
<br/>
========================


If you want to take a step back and learn about Kaldi in general, I have posts on [how to install Kaldi][kaldi-install] or some [miscellaneous Kaldi notes][kaldi-notes] which contain some documentation.

## Introduction

Training a DNN acoustic model does not start the same way as in training a GMM acoustic model.

In the simplest case with GMM-HMM training, we can train a monophone model from a flat start (i.e. from no previous phoneme-to-audio alignments). That means in monophone flat start training, we can begin training a new model with simple utterance-level transcriptions. 

In DNN training, on the otherhand, we typically start with phoneme-to-audio alignements which were generated by an HMM-GMM system. 

A neural net at the end of the day is a tool for classification. We want to be able to take some new features (i.e. audio features) and assign a class to those features (i.e. a phoneme label). So, our acoustic model DNN will have input nodes which correspond to the dimensions of our audio features (think, for example, 39 input nodes for 39 MFCCs), and output nodes which correspond to senome labels (think, 900 output nodes for the 900 context dependent triphones chosen by a decision tree). As you can see, two key parts of the DNN acoustic model (input layer and output layer) are modeled after the features used to train a HMM-GMM and the decision tree created by that GMM-HMM. The dimensions of the hidden layers are up to the researcher / developer.

Once we have a DNN which has the correct number of dimensions for input and output nodes, we can take our phoneme-to-audio alignments and train our neural net. The audio feature frames are fed into the input layer, the net will assign a phoneme label to a frame, and since we already have the gold-standard label for any given frame, we compare what the neural net predicted and what the real phoneme was. Using some loss function and backpropigation, we iterate over all of our training frames to adjust the weights and biases of our net.

At the end, we have something which will hopefully assign the correct phoneme label to a new, unknown audio frame.

{% highlight bash %}
{% endhighlight %}


## Conclusion

I hope this was helpful!

If you have any feedback or questions, don't hesitate to leave a comment!


## Relevant Papers

### Maas et al. (2013) [*Building DNN Acoustic Models for Large Vocabulary Speech Recognition*][maas-2014]

From the abstract:

> Building  neural  network  acoustic  models  requires  several
> design decisions including network architecture, size, and train-
> ing loss function. This paper offers an empirical investigation on
> which aspects of DNN acoustic model design are most important
> for speech recognition system performance. We report DNN clas-
> sifier performance and final speech recognizer word error rates,
> and  compare  DNNs  using  several  metrics  to  quantify  factors
> influencing differences in task performance.


Co-authors include Dan Jurafsky and Andrew Ng, among others. This is a longer paper (22 pages) and is a very thorough 

[kaldi-install]: http://jrmeyer.github.io/kaldi/2016/01/26/Installing-Kaldi.html
[kaldi-notes]: http://jrmeyer.github.io/kaldi/2016/02/01/Kaldi-notes.html
[maas-2014]: https://arxiv.org/pdf/1406.7806.pdf