---
layout: post
title:  "How to Train a Deep Neural Net Acoustic Model with Kaldi"
date:   2016-12-15
categories: kaldi
comments: True
---

<img src="/misc/kaldi_text_and_logo.png" align="right" alt="logo" style="width: 400px;"/>

<br/>
<br/>
<br/>
<br/>

========================
<br/> 
THIS POST IS IN PROGRESS
<br/>
========================


If you want to take a step back and learn about Kaldi in general, I have posts on [how to install Kaldi][kaldi-install] or some [miscellaneous Kaldi notes][kaldi-notes] which contain some documentation.

## Introduction

I've you reading this, you've probably already trained a standard GMM-HMM acoustic model. 

If you haven't already, you should probably do that first, because if you're planning on following the standard Kaldi scripts for DNN training, you need to train a GMM-HMM first.

Ih you have already trained a GMM-HMM model, you should know right off-the-bat that training a DNN acoustic model does not start the same way as in training a GMM acoustic model.

In the simplest case with GMM-HMM training, we train a monophone model from a flat start (i.e. from no previous phoneme-to-audio alignments). That means we can begin training a new model with just utterance-level transcriptions. 

In DNN training, on the otherhand, we typically start training not immediately from utterance-level transcriptions, but from the labeled frames (phoneme-to-audio alignements) which were generated by an GMM-HMM system. This means, your DNN will be greatly affected by the quality of the GMM-HMM you previously trained. A bad GMM-HMM will give you bad alignments, and if you train a DNN with bad alignments, you will have a bad DNN at the end of the day, no matter how many epochs you run, what kind of cost function you use, or how clever your learning rate is.

A neural net at the end of the day is a tool for classification. We want to be able to take some new features (i.e. audio features) and assign a class to those features (i.e. a phoneme label). So, our acoustic model DNN will have input nodes which correspond to the dimensions of our audio features (think, for example, 39 input nodes for 39 MFCCs), and output nodes which correspond to senome labels (think, 900 output nodes for 900 context dependent triphones (decision tree leaves)). As you can see, two key parts of the DNN acoustic model (input layer and output layer) are modeled after the features used to train a GMM-HMM and the decision tree created by that GMM-HMM. The dimensions of the hidden layers are up to the researcher / developer.

Once we have a DNN which has the correct number of dimensions for input and output nodes, we can take our phoneme-to-audio alignments and train our neural net. The audio feature frames are fed into the input layer, the net will assign a phoneme label to a frame, and since we already have the gold-standard label (i.e. phoneme label from our GMM-HMM alignments) for any given frame, we compare what the neural net predicted and what the real phoneme was. Using some loss function and backpropigation, we iterate over all of our training frames to adjust the weights and biases of our net.

Note that unlike GMM-HMM training where in the EM algorithm we realign our transcriptions to our audio frames, with DNN training we don't have to do that. We can, but it's not necessary.

At the end, we hopefully end up with a DNN which will assign the correct phoneme label to a new, unknown audio frame.


## Training a Deep Neural Net

### README

When I start learning a new method, I begin with the most basic, well-documented code I can find. 

So, when approaching Kaldi's DNN code, I chose to start with the **nnet2** code, because even though **nnet3** is newer, the scripts for **nnet2** have been used more, reviewed more, and there's more documentation. 

Then, I chose to start with the **run_nnet2_baseline.sh** script from the Wall Street Journal **egs** directory. This script is located at **kaldi/egs/wsj/s5/local/online/run_nnet2_baseline.sh**. This is, as far as I can gather, the simplest DNN training script for Kaldi at the present moment.

### First Things First: train a GMM system and generate alignments

I'm not going to go into detail on how to train a GMM system. 

However, before you can start training your DNN, you will need the following directories, all generated as part of normal GMM-HMM training in Kaldi: 

1. a training data dir (as generated by a **prepare_data.sh** script in a **s5/local** directory)
2. a language dir (which has information on your phones, decision tree, etc, probably generated by **prepare_lang.sh**)
3. an alignment dir (generated by something like **align_si.sh**).
4. a feature dir (for example MFCCs; made by the **make_mfcc.sh** script)

Here's the specific files you need from these four dirs:


{% highlight bash %}
## DEPENDENCIES FROM GMM-HMM SYSTEM ##

# DATA DIR FILES
$data_dir/feats.scp
$data_dir/cmvn.scp                 # assuming you did CMVN transform in GMM training
$data_dir/utt2spk 
$data_dir/utt2dur
$data_dir/splitJOBN                # where JOB is the total number of JOBs, there's only one dir here (eg. split4)
                   /JOB            # one dir for each JOB, up to JOBN
                        /feats.scp
                        /cmvn.scp
                        /utt2spk

# LANGUAGE DIR FILES
$lang_dir/topo
$lang_dir/oov.int
$lang_dir/phones/silence.csl

# ALIGN DIR FILES
$ali_dir/ali.JOB.gz                     # for as many JOBs as you ran
$ali_dir/final.mdl
$ali_dir/tree
$ali_dir/num_jobs

# MFCC DIR FILES
$mfcc_dir/raw_mfcc_train.JOB.{ark,scp}  # for as many JOBs as you ran
{% endhighlight %}


Because I like visuals and beating dead horses, here's the tree structure for the dirs and files you need from your GMM:


For the **data** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree train/
train/
├── cmvn.scp
├── feats.scp
├── split4
│   ├── 1
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 2
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   ├── 3
│   │   ├── cmvn.scp
│   │   ├── feats.scp
│   │   └── utt2spk
│   └── 4
│       ├── cmvn.scp
│       ├── feats.scp
│       └── utt2spk
├── utt2dur
└── utt2spk

5 directories, 16 files
{% endhighlight %}


For the **lang** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/data$ tree lang/
lang/
├── oov.int
├── phones
│   └── silence.csl
└── topo

1 directory, 3 files
{% endhighlight %}


For the **align** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree triphones_lda_mllt_sat_aligned/
triphones_lda_mllt_sat_aligned/
├── ali.1.gz
├── ali.2.gz
├── ali.3.gz
├── ali.4.gz
├── final.mat
├── final.mdl
├── num_jobs
└── tree

0 directories, 8 files
{% endhighlight %}


For the **mfcc** dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model$ tree mfcc/
mfcc/
├── raw_mfcc_train.1.ark
├── raw_mfcc_train.1.scp
├── raw_mfcc_train.2.ark
├── raw_mfcc_train.2.scp
├── raw_mfcc_train.3.ark
├── raw_mfcc_train.3.scp
├── raw_mfcc_train.4.ark
└── raw_mfcc_train.4.scp

0 directories, 8 files
{% endhighlight %}

### The Main Run Script: **run_nnet2_toy_example.sh**

I've adjusted the this main run script (as well as it's dependencies) to work for my own data set. Originally, I took **run_nnet2_baseline.sh** and simplified it as much as possible, changed the normal **pnorm** nonlinearity to more basic **tanh**, and removed online preconditioning (which is used to find better learning rates).

In general, I've deleted as much as I could of the original script along the way, trying to get to the bare bones of the standard Kaldi neural net training pipeline. The original authors of the scripts had added in lots of good, practical features for training and testing, but as of right now, I'm only concerned with the core of the training procedure. Keep this in mind, because your results will be far from state-of-the-art.

As you may have noticed in other **run.sh** scripts, this script mainly defines variables and then sends off the heavy lifting of data preparation, training, and testing to other, more specialized scripts. 

With this is mind, here's the beginning of my modified **run_nnet2_toy_example.sh** script, which defines some variables:


{% highlight bash %}
stage=1
train_stage=-10
experiment_dir=experiment/nnet2/nnet2_toy_example
num_threads=4
minibatch_size=128
parallel_opts="-pe smp $num_threads"
unknown_phone=SPOKEN_NOISE # having these explicit is just something I did when
silence_phone=SIL          # I was debugging, they are now required by decode.sh


. ./path.sh
. ./utils/parse_options.sh
{% endhighlight%}


Next, now that we've set our general variables, we have the main code to call the training script. In the original **run_nnet2_baseline.sh**, the training script called here was **train_pnorm_simple.sh**. I've modified this training script heavily, and maybe most importantly I replaced the **pnorm** non-linearity with the simpler **tanh** function. As such, I've changed the name from **train_pnorm_simple.sh** to **train_simple.sh**. 

So, our main run script will call our main train script as such:

{% highlight bash %}

if [ $stage -le 1 ]; then

    echo ""
    echo "######################"
    echo "### BEGIN TRAINING ###"
    echo "######################"

    steps/nnet2/train_simple.sh \
        --stage $train_stage \
        --parallel-opts "$parallel_opts" \
        --num-threads "$num_threads" \
        --num-jobs-nnet 4 \
        --feat-type raw \
        --cmvn-opts "--norm-means=false --norm-vars=false" \
        --splice-width 4 \
        --lda_dim 40 \
        --num-hidden-layers 3 \
        --hidden-layer-dim 100 \
        --add-layers-period 2 \
        --num-epochs 15 \
        --mix-up 2000 \
        --initial-learning-rate 0.02 \
        --final-learning-rate 0.004 \
        --minibatch-size "$minibatch_size" \
        data/train \
        data/lang \
        experiment/triphones_lda_mllt_sat_aligned \
        $experiment_dir \
        || exit 1;

    echo ""
    echo "####################"
    echo "### END TRAINING ###"
    echo "####################"

fi
{% endhighlight %}

As you can see, the only obligatory arguments for **train_simple.sh** are:

1. the training data
2. the language dir
3. our alignments from our previous GMM-HMM model
4. the name of the dir where we will save our new DNN model

So far this is pretty straightforward: you feed in the right data to the script and you get back a nice shiny DNN. At this point, you could keep this **train_simple.sh** script a black box, and not worry about what's going on inside, or you could dive into it. I'm going to take the time to go through it here for anyone who's interested, but if you aren't interested you can skip down below.

### The Main Train Script: **train_simple.sh**

First, we have a bunch of default parameters to set:

{% highlight bash %}
#!/bin/bash

# Copyright 2012-2014  Johns Hopkins University (Author: Daniel Povey). 
#           2013  Xiaohui Zhang
#           2013  Guoguo Chen
#           2014  Vimal Manohar
# Apache 2.0.

# Final model-combination: we combine models over typically a whole epoch, 
# and because that would be too many iterations to
# easily be able to combine over, we arrange the iterations into groups (20
# groups by default) and average over each group.
#

# Begin configuration section.
cmd=run.pl
stage=-4
num_epochs=15      # Number of epochs of training;
                   # the number of iterations is worked out from this.
initial_learning_rate=0.04
final_learning_rate=0.004
bias_stddev=0.5
hidden_layer_dim=100
minibatch_size=128 # by default use a smallish minibatch size for neural net
                   # training; this controls instability which would otherwise
                   # be a problem with multi-threaded update. 
samples_per_iter=400000 # each iteration of training, see this many samples
                        # per job.  This option is passed to get_egs.sh
num_jobs_nnet=16   # Number of neural net jobs to run in parallel.  This option
                   # is passed to get_egs.sh.
online_ivector_dir=
max_models_combine=20 # The "max_models_combine" is the maximum number of models we give
  # to the final 'combine' stage, but these models will themselves be averages of
  # iteration-number ranges.
shuffle_buffer_size=5000 # This "buffer_size" variable controls randomization of the samples
                # on each iter.  You could set it to 0 or to a large value for complete
                # randomization, but this would both consume memory and cause spikes in
                # disk I/O.  Smaller is easier on disk and memory but less random.  It's
                # not a huge deal though, as samples are anyway randomized right at the start.
                # (the point of this is to get data in different minibatches on different iterations,
                # since in the preconditioning method, 2 samples in the same minibatch can
                # affect each others' gradients.
add_layers_period=2 # by default, add new layers every 2 iterations.
num_hidden_layers=3
io_opts="-tc 5" # for jobs with a lot of I/O, limits the number running at one time.
splice_width=4 # meaning +- 4 frames on each side for second LDA
randprune=4.0 # speeds up LDA.
max_change_per_sample=0.075
mix_up=0 # Number of components to mix up to (should be > #tree leaves, if  specified.)
num_threads=16
parallel_opts="--num-threads 16 --mem 1G" 
  # by default we use 16 threads; this lets the queue know.
  # note: parallel_opts doesn't automatically get adjusted if you adjust num-threads.
combine_num_threads=8
combine_parallel_opts="--num-threads 16"  # queue options for "combine" stage.
cleanup=true
lda_dim=40
transform_dir=     # If supplied, overrides ali_dir
cmvn_opts=  # will be passed to get_lda.sh and get_egs.sh, if supplied.  
            # only relevant for "raw" features, not lda.
feat_type=  # Can be used to force "raw" features.
prior_subset_size=10000 # 10k samples per job, for computing priors.  Should be
                        # more than enough.

echo "$0 $@"  # Print the command line for logging

. ./path.sh || exit 1; # make sure we have a path.sh script
. parse_options.sh || exit 1;
{% endhighlight %}

After parsing the command-line arguments, we check to make sure our important files are where they should be. All these files should have been generated during our GMM-HMM training.

{% highlight bash %}
data_dir=$1
lang_dir=$2
ali_dir=$3
exp_dir=$4

# Check some files from our GMM-HMM system
for f in \
    $data_dir/feats.scp \
    $data_dir/utt2spk \
    $lang_dir/topo \
    $lang_dir/oov.int \
    $lang_dir/phones/silence.csl \
    $ali_dir/ali.1.gz \
    $ali_dir/final.mdl \
    $ali_dir/tree \
    $ali_dir/num_jobs;
    do [ ! -f $f ] && echo "$0: no such file $f" && exit 1;
done
{% endhighlight %}


Once we're sure all the important files are in place, we extract some variables from the information in those files:


{% highlight bash %}
# Set number of leaves
num_leaves=`tree-info $ali_dir/tree 2>/dev/null | grep num-pdfs | awk '{print $2}'` || exit 1;
[ -z $num_leaves ] && echo "\$num_leaves is unset" && exit 1;
[ "$num_leaves" -eq "0" ] && echo "\$num_leaves is 0" && exit 1;

# set up some dirs and parameter definition files
nj=`cat $ali_dir/num_jobs` || exit 1;
sdata=$data_dir/split$nj
utils/split_data.sh $data_dir $nj
mkdir -p $exp_dir/log
echo $nj > $exp_dir/num_jobs
cp $ali_dir/tree $exp_dir

# set some extra options
extra_opts=()
[ ! -z "$cmvn_opts" ] && extra_opts+=(--cmvn-opts "$cmvn_opts")
[ ! -z "$feat_type" ] && extra_opts+=(--feat-type $feat_type)
[ ! -z "$online_ivector_dir" ] && extra_opts+=(--online-ivector-dir $online_ivector_dir)
[ -z "$transform_dir" ] && transform_dir=$ali_dir
extra_opts+=(--transform-dir $transform_dir)
extra_opts+=(--splice-width $splice_width)
{% endhighlight %}


At this point in the script, we have defined a bunch of variables, created two files (1) **tree** (copied from the GMM-HMM) and (2) **num_jobs**, and created an empty **log** directory. We can see these new additions in our main experiment dir:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2_online/
nnet2_online/
└── nnet_a_baseline
    ├── log
    ├── num_jobs
    └── tree

2 directories, 2 files
{% endhighlight %}


Now we move on to data preparation before training, and we start with getting the LDA feature transform (via **get_lda.sh**) we will use before we take our features as input to our DNN. Remember, we're still in **train_simple.sh** at this point.

{% highlight bash %}
if [ $stage -le -4 ]; then

    echo ""
    echo "########################"
    echo "### BEGIN get_lda.sh ###"
    echo "########################"

    steps/nnet2/get_lda.sh \
        --cmd "$cmd" \
        --lda-dim $lda_dim \
        "${extra_opts[@]}" \
        $data_dir \
        $lang_dir \
        $ali_dir \
        $exp_dir \
        || exit 1;

    # these files should have been written by get_lda.sh
    feat_dim=$(cat $exp_dir/feat_dim) || exit 1;
    ivector_dim=$(cat $exp_dir/ivector_dim) || exit 1;
    lda_dim=$(cat $exp_dir/lda_dim) || exit 1;
    lda_mat=$exp_dir/lda.mat || exit;

fi
{% endhighlight %}


This script outputs a matrix for the LDA transform, and this very matrix will come up again when we initialize the neural net as a **FixedAffineComponent**, just after our input layer with splicing. That means, once we've got our LDA transform, it will applied to all input, and because it is a **FixedComponent**, the matrix will not be updated by back-propagation.

Here are the new files created by **get_lda.sh**.

{% highlight bash %}
# At this point, we have generated the following:
# josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2/
# nnet2/
# └── nnet2_toy_example
#     ├── cholesky.tpmat
#     ├── cmvn_opts
#     ├── feat_dim
#     ├── ivector_dim
#     ├── lda.acc
#     ├── lda_dim
#     ├── lda.mat
#     ├── log
#     │   ├── lda_acc.1.log
#     │   ├── lda_acc.2.log
#     │   ├── lda_acc.3.log
#     │   ├── lda_acc.4.log
#     │   ├── lda_est.log
#     │   └── lda_sum.log
#     └── within_covar.spmat

# 2 directories, 14 files
{% endhighlight %}

Now that we've estimated the LDA transform which we will later apply to our spliced feature frames in training, we will split up our data into the appropriate directories for training and validation:

{% highlight bash  %}
if [ $stage -le -3 ]; then

    echo ""
    echo "########################"
    echo "### BEGIN get_egs.sh ###"
    echo "########################"

    steps/nnet2/get_egs.sh \
        --cmd "$cmd" \
        "${extra_opts[@]}" \
        --samples-per-iter $samples_per_iter \
        --num-jobs-nnet $num_jobs_nnet \
        --io-opts "$io_opts" \
        $data_dir \
        $ali_dir \
        $exp_dir \
        || exit 1;

    # this is the path to the new egs dir that was just created
    egs_dir=$exp_dir/egs

fi
{% endhighlight %}

After we run the **get_egs.sh** script, we find that we have generated 27 new files and one new **egs** dir.

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2/
nnet2/
└── nnet_toy_example
    ├── egs
    │   ├── combine.egs
    │   ├── egs.1.0.ark
    │   ├── egs.2.0.ark
    │   ├── egs.3.0.ark
    │   ├── egs.4.0.ark
    │   ├── iters_per_epoch
    │   ├── num_jobs_nnet
    │   ├── samples_per_iter
    │   ├── train_diagnostic.egs
    │   └── valid_diagnostic.egs
    ├── log
    │   ├── create_train_subset_combine.log
    │   ├── create_train_subset_diagnostic.log
    │   ├── create_train_subset.log
    │   ├── create_valid_subset_combine.log
    │   ├── create_valid_subset_diagnostic.log
    │   ├── create_valid_subset.log
    │   ├── get_egs.1.log
    │   ├── get_egs.2.log
    │   ├── get_egs.3.log
    │   ├── get_egs.4.log
    │   ├── shuffle.0.1.log
    │   ├── shuffle.0.2.log
    │   ├── shuffle.0.3.log
    │   └── shuffle.0.4.log
    ├── num_frames
    ├── train_subset_uttlist
    └── valid_uttlist

3 directories, 27 files
{% endhighlight %}

Now that we have the training examples (phone-to-frame alignments) sorted and in the correct format, we go on to initialize our neural net.

Similar to our **topo** configuration file we used in GMM-HMM training, we have to define the dimensions and architecture of our neural net before we can initialize it. These definitions are stored in a configuration file called **nnet.config**, and it gets defined and saved to disk as a normal part of the **train_simple.sh** script.


{% highlight bash %}
    cat >$exp_dir/nnet.config <<EOF
SpliceComponent input-dim=$tot_input_dim left-context=$splice_width right-context=$splice_width const-component-dim=$ivector_dim
FixedAffineComponent matrix=$lda_mat
AffineComponent input-dim=$lda_dim output-dim=$hidden_layer_dim learning-rate=$initial_learning_rate param-stddev=$stddev bias-stddev=$bias_stddev
TanhComponent dim=$hidden_layer_dim
AffineComponent input-dim=$hidden_layer_dim output-dim=$num_leaves learning-rate=$initial_learning_rate param-stddev=0 bias-stddev=0
SoftmaxComponent dim=$num_leaves
EOF
{% endhighlight %}

*The below descriptions are taken almost verbatim from the official Kaldi [nnet2 docs][nnet2-docs].*

1. **SpliceComponent** defines the size of the window of feature-frame-splicing to perform. 
2. **FixedAffineComponent** is our LDA-like transform created by **get_lda.sh**.
3. **AffineComponent** is the standard **Wx+b** affine transform found in neural nets. This first **AffineComponent** represents the weights and biases between the input layer and the first hidden layer.
4. **TanhComponent** is the standard **tanh** nonlinearity.
5. **AffineComponent** is the standard **Wx+b** affine transform found in neural nets. This second **AffineComponent** represents the weights and biases between the hidden layer and the output layer.
6. **SoftmaxComponent** is the final nonlinearity that produces properly normalized probabilities at the output.


This initial DNN configuration contains one and only one hidden layer. 

That is, there are six Kaldi *components*, but only three layers in the net. It follows that there are only two weight matrices and two bias vectors. If we look back at the **nnet.config** file definition, there are indeed only two components which are updatable, both are of the form **AffineComponent**.

Next in the **train_simple.sh** script, we create another, similar configuration file which defines the kind of hidden layers we will be adding to our net during training:


{% highlight bash %}
    cat >$exp_dir/hidden.config <<EOF
AffineComponent input-dim=$hidden_layer_dim output-dim=$hidden_layer_dim learning-rate=$initial_learning_rate param-stddev=$stddev bias-stddev=$bias_stddev
TanhComponent dim=$hidden_layer_dim
EOF
{% endhighlight %}

Again, we find here an affine transform followed by a non-linearity. Nothing too crazy.

Now, we can put our decision tree, HMM topology file, and **nnet.config** file to work and initialize our first neural net, aka **0.mdl**:

{% highlight bash %}
$cmd $exp_dir/log/nnet_init.log \
    nnet-am-init \
        $ali_dir/tree \
        $lang_dir/topo \
        "nnet-init $exp_dir/nnet.config -|" \
        $exp_dir/0.mdl \
        || exit 1;
{% endhighlight %}


Let's do another "check-in" to see what files we had created. We find that we've only created four new files:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment$ tree nnet2/
nnet2/
└── nnet2_toy_example
    ├── 0.mdl
    ├── hidden.config
    ├── log
    │   └── nnet_init.log
    └── nnet.config

2 directories, 4 files
{% endhighlight %}


Also, we can take a look at our untrained model and get some info about it by making use of the nifty **nnet-am-info.cc** program:


{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment/nnet2/nnet2_toy_example$ ../../../../../../src/nnet2bin/nnet-am-info 0.mdl
../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
num-components 6
num-updatable-components 2
left-context 4
right-context 4
input-dim 13
output-dim 1759
parameter-dim 181759
component 0 : SpliceComponent, input-dim=13, output-dim=117, context=-4 -3 -2 -1 0 1 2 3 4 
component 1 : FixedAffineComponent, input-dim=117, output-dim=40, linear-params-stddev=0.0146923, bias-params-stddev=2.91086
component 2 : AffineComponent, input-dim=40, output-dim=100, linear-params-stddev=0.100784, bias-params-stddev=0.49376, learning-rate=0.02
component 3 : TanhComponent, input-dim=100, output-dim=100
component 4 : AffineComponent, input-dim=100, output-dim=1759, linear-params-stddev=0, bias-params-stddev=0, learning-rate=0.02
component 5 : SoftmaxComponent, input-dim=1759, output-dim=1759
prior dimension: 0
{% endhighlight %}


Now that we have an initialized model and labeled training examples to go along, we can train the transitions of the HMMs in our DNN-HMM acoustic model. In GMM-HMM training the transitions get updated during EM training, but since we are not doing any realignment for DNN training, the initial transitional probabilities will do just fine.


{% highlight bash %}
$cmd $exp_dir/log/train_trans.log \
    nnet-train-transitions \
        $exp_dir/0.mdl \
        "ark:gunzip -c $ali_dir/ali.*.gz|" \
        $exp_dir/0.mdl \
        || exit 1;
{% endhighlight %}

Since we are seeding with the original **0.mdl** and resaving it as the same name **0.mdl**, the only file produced by this command is the log file **train_trans.log**.

Taking another quote from the offical [nnet2 docs][nnet2-docs] with regards to **nnet-train-transitions.cc**:

>This computes the transition probabilities that will be used in the HMMs in decoding (which has nothing to do 
>with the neural net itself), and also computes the prior probabilities of the "targets" (the several thousand 
>context-dependent states). Later, when we do decoding, we will divide the posteriors computed by the network
>by these priors to get "pseudo-likelihoods"; these are more compatible with the HMM framework than raw posteriors.

These priors are stored back in the model, and we can see that they exist by again getting info about our neural net with **nnet-am-info.cc**

{% highlight bash %}
josh@yoga:~/git/kaldi/egs/kgz/kyrgyz-model/experiment/nnet2/nnet2_toy_example$ ../../../../../../src/nnet2bin/nnet-am-info 0.mdl
../../../../../../src/nnet2bin/nnet-am-info 0.mdl 
num-components 6
num-updatable-components 2
left-context 4
right-context 4
input-dim 13
output-dim 1759
parameter-dim 181759
component 0 : SpliceComponent, input-dim=13, output-dim=117, context=-4 -3 -2 -1 0 1 2 3 4 
component 1 : FixedAffineComponent, input-dim=117, output-dim=40, linear-params-stddev=0.0146923, bias-params-stddev=2.91086
component 2 : AffineComponent, input-dim=40, output-dim=100, linear-params-stddev=0.100784, bias-params-stddev=0.49376, learning-rate=0.02
component 3 : TanhComponent, input-dim=100, output-dim=100
component 4 : AffineComponent, input-dim=100, output-dim=1759, linear-params-stddev=0, bias-params-stddev=0, learning-rate=0.02
component 5 : SoftmaxComponent, input-dim=1759, output-dim=1759
prior dimension: 1759, prior sum: 1, prior min: 1.68406e-05
{% endhighlight %}

Now we move on to the main training loop, which will update our parameters via backpropagation.

Here's the most important code snippet from this loop:


{% highlight bash %}
$cmd $parallel_opts JOB=1:$num_jobs_nnet $exp_dir/log/train.$x.JOB.log \
     nnet-shuffle-egs \
         --buffer-size=$shuffle_buffer_size \
         --srand=$x \
         ark:$egs_dir/egs.JOB.$[$x%$iters_per_epoch].ark \
         ark:- \| \
     nnet-train-parallel \
         --num-threads=$num_threads \
         --minibatch-size=$this_minibatch_size \
         --srand=$x "$mdl" \
         ark:- \
         $exp_dir/$[$x+1].JOB.mdl \
     || exit 1;
{% endhighlight %}

Copy-and-pasting from the comments of the **nnet-train-parallel.cc** source code:

>Train the neural network parameters with backprop and stochastic
>gradient descent using minibatches.  As nnet-train-simple, but
>uses multiple threads in a Hogwild type of update (for CPU, not GPU).



## Conclusion

I hope this was helpful!

If you have any feedback or questions, don't hesitate to leave a comment!


## Relevant Papers

### Maas et al. (2013) [*Building DNN Acoustic Models for Large Vocabulary Speech Recognition*][maas-2014]

From the abstract:

> Building  neural  network  acoustic  models  requires  several
> design decisions including network architecture, size, and train-
> ing loss function. This paper offers an empirical investigation on
> which aspects of DNN acoustic model design are most important
> for speech recognition system performance. We report DNN clas-
> sifier performance and final speech recognizer word error rates,
> and  compare  DNNs  using  several  metrics  to  quantify  factors
> influencing differences in task performance.


Co-authors include Dan Jurafsky and Andrew Ng, among others. This is a longer paper (22 pages) and is a very thorough 

[kaldi-install]: http://jrmeyer.github.io/kaldi/2016/01/26/Installing-Kaldi.html
[kaldi-notes]: http://jrmeyer.github.io/kaldi/2016/02/01/Kaldi-notes.html
[maas-2014]: https://arxiv.org/pdf/1406.7806.pdf
[nnet2-docs]: http://kaldi-asr.org/doc/dnn2.html